{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liangwei/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/liangwei/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ---- 参数定义---- \n",
    "cell_size=10       # 隐藏层cell个数\n",
    "input_size=7       # 输入层维数\n",
    "time_step = 20     # 步长窗口\n",
    "output_size=1      # 输出层维数\n",
    "lr=0.0006         # 学习率\n",
    "batch_size=60      # 每批次大小\n",
    "split_line=5800 #前5800行作为训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 数据导入 ---- \n",
    "df = pd.read_csv('dataset_2.csv') # 导csv\n",
    "origin = df.iloc[:,2:].values     # #取第3～10列，一共8列，原始数据(6109, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 标准化，工具函数\n",
    "def normal(data):\n",
    "    mean=np.mean(data,axis=0)\n",
    "    std=np.std(data,axis=0)\n",
    "    return (data-mean)/std, mean, std #有三个返回值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集\n",
    "#使用前5800行,其中每个输入的x是(20,7)的矩阵，每个输出的y是(20,1)的矩阵，它是20条x对应的y合成的\n",
    "train,_,_ = normal(origin[:split_line]) # (5800, 8),取normal函数第1个返回值，舍弃后两个返回值\n",
    "x_train,y_train=[],[]\n",
    "for i in range(len(train)//time_step): # i=0~5780\n",
    "    x_train.append(train[i*time_step:(i+1)*time_step,:-1]) #往x_train里放入一个(20,7)的矩阵\n",
    "    y_train.append(train[i*time_step:(i+1)*time_step,-1,np.newaxis]) \n",
    "    #x_train.append( train[i:i+time_step   ,:-1  ] ) #往x_train里放入一个(20,7)的矩阵\n",
    "    #y_train.append( train[i:i+time_step   ,-1   ,np.newaxis] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n",
      "(290, 20, 7)\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(np.array(x_train).shape)\n",
    "\n",
    "#for i in range(len(x_train)):\n",
    "#    print(i, np.array(x_train[i]).shape)\n",
    "#for j in range(len(y_train)):\n",
    "#    print(j, np.array(y_train[j]).shape)\n",
    "\n",
    "#必须要加各种print才可以看出各种矩阵形状的变化\n",
    "#for batch in range( len(x_train)//batch_size+1): # 1个周期内批次数\n",
    "#    start = batch*batch_size \n",
    "#    end = min( (batch+1)*batch_size , len(x_train)) \n",
    "#    batch_xs = x_train[start : end ]# 获得本批次的x,y\n",
    "#    batch_ys = y_train[start : end ]\n",
    "#    print(np.array(batch_xs).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试集\n",
    "test,mean,std= normal(origin[split_line:])\n",
    "x_test,y_test=[],[]\n",
    "for i in range(len(test)//time_step): # (6109-5801)/20 ~= 15\n",
    "    x_test.append(test[i*time_step:(i+1)*time_step,:-1])\n",
    "    #y_test.append(test[i*time_step:(i+1)*time_step,-1])\n",
    "    y_test.extend( test[i*time_step:(i+1)*time_step,-1] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(15, 20, 7)\n",
      "300\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "#for i in range(len(x_test)):\n",
    "#    print(i, np.array(x_test[i]).shape)\n",
    "#for j in range(len(y_test)):\n",
    "#    print(j, np.array(y_test[j]).shape)\n",
    "print(len(x_test))\n",
    "print(np.array(x_test).shape)\n",
    "print(len(y_test))\n",
    "print(np.array(y_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义网络\n",
    "#X就是输入的数据张量Tensor了，形状为(#data_example, time_step, input_size)的Tensor\n",
    "X=tf.placeholder(tf.float32, shape=[None,time_step,input_size]) # (Any, 20, 7)\n",
    "Y=tf.placeholder(tf.float32, shape=[None,time_step,output_size]) # (Any, 20, 1)\n",
    "\n",
    "#输入层权重、偏置\n",
    "w1 = tf.Variable(tf.random_normal([input_size,cell_size])) # (7,10)\n",
    "b1 = tf.Variable(tf.constant(0.1,shape=[cell_size,])) # (10,1)\n",
    "\n",
    "#输出层权重、偏置\n",
    "w2 = tf.Variable(tf.random_normal([cell_size,1]))# (10,1)\n",
    "b2 = tf.Variable(tf.constant(0.1,shape=[1,]))# (1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#定义LSTM\n",
    "def RNN(x):\n",
    "    input=tf.reshape(x,[-1,input_size]) # x降维\n",
    "    input_rnn=tf.matmul(input,w1)+b1\n",
    "    input_rnn=tf.reshape(input_rnn,[-1,time_step,cell_size])  #将tensor转成3维，作为lstm cell的输入\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(cell_size) #定义LSTM， block数为cell_size\n",
    "    output, final_state = tf.nn.dynamic_rnn(cell, input_rnn, dtype=tf.float32)\n",
    "    output = tf.reshape(output,[-1,cell_size]) \n",
    "    result = tf.matmul(output,w2)+b2\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义TensorFlow计算\n",
    "prediction = RNN(X) # 预测\n",
    "loss = tf.reduce_mean(tf.square(tf.reshape(prediction,[-1])-tf.reshape(Y, [-1]))) # 误差 mean_square(prediction,Y)\n",
    "train_op=tf.train.AdamOptimizer(lr).minimize(loss) # 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 0  loss: 0.43035868\n",
      "Number of iterations: 1  loss: 0.38651952\n",
      "Number of iterations: 2  loss: 0.34302598\n",
      "Number of iterations: 3  loss: 0.30268532\n",
      "Number of iterations: 4  loss: 0.2665273\n",
      "Number of iterations: 5  loss: 0.23498084\n",
      "Number of iterations: 6  loss: 0.20812462\n",
      "Number of iterations: 7  loss: 0.18577242\n",
      "Number of iterations: 8  loss: 0.16752563\n",
      "Number of iterations: 9  loss: 0.15282807\n",
      "Number of iterations: 10  loss: 0.14103442\n",
      "Number of iterations: 11  loss: 0.13148347\n",
      "Number of iterations: 12  loss: 0.12356163\n",
      "Number of iterations: 13  loss: 0.116744034\n",
      "Number of iterations: 14  loss: 0.11060646\n",
      "Number of iterations: 15  loss: 0.104820184\n",
      "Number of iterations: 16  loss: 0.09915874\n",
      "Number of iterations: 17  loss: 0.09352159\n",
      "Number of iterations: 18  loss: 0.087931044\n",
      "Number of iterations: 19  loss: 0.08247987\n",
      "Number of iterations: 20  loss: 0.077268474\n",
      "Number of iterations: 21  loss: 0.072370194\n",
      "Number of iterations: 22  loss: 0.06782744\n",
      "Number of iterations: 23  loss: 0.063658275\n",
      "Number of iterations: 24  loss: 0.059862226\n",
      "Number of iterations: 25  loss: 0.056423217\n",
      "Number of iterations: 26  loss: 0.053312764\n",
      "Number of iterations: 27  loss: 0.05049512\n",
      "Number of iterations: 28  loss: 0.047933184\n",
      "Number of iterations: 29  loss: 0.04559372\n",
      "Number of iterations: 30  loss: 0.043450274\n",
      "Number of iterations: 31  loss: 0.041482933\n",
      "Number of iterations: 32  loss: 0.039676774\n",
      "Number of iterations: 33  loss: 0.038020242\n",
      "Number of iterations: 34  loss: 0.0365039\n",
      "Number of iterations: 35  loss: 0.035119507\n",
      "Number of iterations: 36  loss: 0.033858716\n",
      "Number of iterations: 37  loss: 0.032711916\n",
      "Number of iterations: 38  loss: 0.03166789\n",
      "Number of iterations: 39  loss: 0.03071453\n",
      "Number of iterations: 40  loss: 0.029839901\n",
      "Number of iterations: 41  loss: 0.029033108\n",
      "Number of iterations: 42  loss: 0.028284913\n",
      "Number of iterations: 43  loss: 0.02758785\n",
      "Number of iterations: 44  loss: 0.026936226\n",
      "Number of iterations: 45  loss: 0.026325947\n",
      "Number of iterations: 46  loss: 0.025754068\n",
      "Number of iterations: 47  loss: 0.025218185\n",
      "Number of iterations: 48  loss: 0.024715863\n",
      "Number of iterations: 49  loss: 0.024244303\n",
      "Number of iterations: 50  loss: 0.023800194\n",
      "Number of iterations: 51  loss: 0.023379708\n",
      "Number of iterations: 52  loss: 0.022978717\n",
      "Number of iterations: 53  loss: 0.022592988\n",
      "Number of iterations: 54  loss: 0.022218525\n",
      "Number of iterations: 55  loss: 0.021851826\n",
      "Number of iterations: 56  loss: 0.021490123\n",
      "Number of iterations: 57  loss: 0.021131422\n",
      "Number of iterations: 58  loss: 0.020774517\n",
      "Number of iterations: 59  loss: 0.020418769\n",
      "Number of iterations: 60  loss: 0.020063965\n",
      "Number of iterations: 61  loss: 0.019710165\n",
      "Number of iterations: 62  loss: 0.019357478\n",
      "Number of iterations: 63  loss: 0.019006059\n",
      "Number of iterations: 64  loss: 0.01865602\n",
      "Number of iterations: 65  loss: 0.01830747\n",
      "Number of iterations: 66  loss: 0.017960494\n",
      "Number of iterations: 67  loss: 0.017615238\n",
      "Number of iterations: 68  loss: 0.017271863\n",
      "Number of iterations: 69  loss: 0.016930582\n",
      "Number of iterations: 70  loss: 0.016591651\n",
      "Number of iterations: 71  loss: 0.016255353\n",
      "Number of iterations: 72  loss: 0.015921993\n",
      "Number of iterations: 73  loss: 0.015591858\n",
      "Number of iterations: 74  loss: 0.015265219\n",
      "Number of iterations: 75  loss: 0.014942347\n",
      "Number of iterations: 76  loss: 0.014623478\n",
      "Number of iterations: 77  loss: 0.014308826\n",
      "Number of iterations: 78  loss: 0.013998606\n",
      "Number of iterations: 79  loss: 0.013693018\n",
      "Number of iterations: 80  loss: 0.013392264\n",
      "Number of iterations: 81  loss: 0.013096544\n",
      "Number of iterations: 82  loss: 0.012806053\n",
      "Number of iterations: 83  loss: 0.012520982\n",
      "Number of iterations: 84  loss: 0.012241512\n",
      "Number of iterations: 85  loss: 0.011967823\n",
      "Number of iterations: 86  loss: 0.011700071\n",
      "Number of iterations: 87  loss: 0.01143839\n",
      "Number of iterations: 88  loss: 0.011182901\n",
      "Number of iterations: 89  loss: 0.010933689\n",
      "Number of iterations: 90  loss: 0.010690831\n",
      "Number of iterations: 91  loss: 0.010454358\n",
      "Number of iterations: 92  loss: 0.010224287\n",
      "Number of iterations: 93  loss: 0.0100006135\n",
      "Number of iterations: 94  loss: 0.009783308\n",
      "Number of iterations: 95  loss: 0.009572324\n",
      "Number of iterations: 96  loss: 0.009367594\n",
      "Number of iterations: 97  loss: 0.009169034\n",
      "Number of iterations: 98  loss: 0.008976553\n",
      "Number of iterations: 99  loss: 0.008790046\n",
      "Number of iterations: 100  loss: 0.008609398\n",
      "Number of iterations: 101  loss: 0.008434487\n",
      "Number of iterations: 102  loss: 0.00826519\n",
      "Number of iterations: 103  loss: 0.008101374\n",
      "Number of iterations: 104  loss: 0.007942893\n",
      "Number of iterations: 105  loss: 0.0077896123\n",
      "Number of iterations: 106  loss: 0.0076413937\n",
      "Number of iterations: 107  loss: 0.00749809\n",
      "Number of iterations: 108  loss: 0.0073595573\n",
      "Number of iterations: 109  loss: 0.007225646\n",
      "Number of iterations: 110  loss: 0.0070962184\n",
      "Number of iterations: 111  loss: 0.006971131\n",
      "Number of iterations: 112  loss: 0.006850235\n",
      "Number of iterations: 113  loss: 0.0067334045\n",
      "Number of iterations: 114  loss: 0.0066204946\n",
      "Number of iterations: 115  loss: 0.0065113795\n",
      "Number of iterations: 116  loss: 0.0064059254\n",
      "Number of iterations: 117  loss: 0.0063040215\n",
      "Number of iterations: 118  loss: 0.0062055225\n",
      "Number of iterations: 119  loss: 0.006110344\n",
      "Number of iterations: 120  loss: 0.0060182903\n",
      "Number of iterations: 121  loss: 0.005929389\n",
      "Number of iterations: 122  loss: 0.0058432734\n",
      "Number of iterations: 123  loss: 0.005760234\n",
      "Number of iterations: 124  loss: 0.0056794723\n",
      "Number of iterations: 125  loss: 0.0056020105\n",
      "Number of iterations: 126  loss: 0.005525838\n",
      "Number of iterations: 127  loss: 0.005454048\n",
      "Number of iterations: 128  loss: 0.0053811893\n",
      "Number of iterations: 129  loss: 0.005316409\n",
      "Number of iterations: 130  loss: 0.005244619\n",
      "Number of iterations: 131  loss: 0.0051932563\n",
      "Number of iterations: 132  loss: 0.0051236837\n",
      "Number of iterations: 133  loss: 0.0051011695\n",
      "Number of iterations: 134  loss: 0.005018398\n",
      "Number of iterations: 135  loss: 0.0049653044\n",
      "Number of iterations: 136  loss: 0.004888667\n",
      "Number of iterations: 137  loss: 0.004834016\n",
      "Number of iterations: 138  loss: 0.004794833\n",
      "Number of iterations: 139  loss: 0.0047298595\n",
      "Number of iterations: 140  loss: 0.0046827267\n",
      "Number of iterations: 141  loss: 0.004633393\n",
      "Number of iterations: 142  loss: 0.004583136\n",
      "Number of iterations: 143  loss: 0.004539304\n",
      "Number of iterations: 144  loss: 0.0044910354\n",
      "Number of iterations: 145  loss: 0.004446414\n",
      "Number of iterations: 146  loss: 0.00440282\n",
      "Number of iterations: 147  loss: 0.004358729\n",
      "Number of iterations: 148  loss: 0.0043171556\n",
      "Number of iterations: 149  loss: 0.0042753695\n",
      "Number of iterations: 150  loss: 0.0042347694\n",
      "Number of iterations: 151  loss: 0.0041951523\n",
      "Number of iterations: 152  loss: 0.004155887\n",
      "Number of iterations: 153  loss: 0.0041177645\n",
      "Number of iterations: 154  loss: 0.004080158\n",
      "Number of iterations: 155  loss: 0.004043326\n",
      "Number of iterations: 156  loss: 0.004007243\n",
      "Number of iterations: 157  loss: 0.003971733\n",
      "Number of iterations: 158  loss: 0.003936972\n",
      "Number of iterations: 159  loss: 0.0039027983\n",
      "Number of iterations: 160  loss: 0.0038692674\n",
      "Number of iterations: 161  loss: 0.003836353\n",
      "Number of iterations: 162  loss: 0.0038040148\n",
      "Number of iterations: 163  loss: 0.003772275\n",
      "Number of iterations: 164  loss: 0.0037410879\n",
      "Number of iterations: 165  loss: 0.0037104585\n",
      "Number of iterations: 166  loss: 0.0036803668\n",
      "Number of iterations: 167  loss: 0.0036507987\n",
      "Number of iterations: 168  loss: 0.0036217452\n",
      "Number of iterations: 169  loss: 0.003593189\n",
      "Number of iterations: 170  loss: 0.0035651212\n",
      "Number of iterations: 171  loss: 0.0035375273\n",
      "Number of iterations: 172  loss: 0.0035103997\n",
      "Number of iterations: 173  loss: 0.003483721\n",
      "Number of iterations: 174  loss: 0.0034574848\n",
      "Number of iterations: 175  loss: 0.003431676\n",
      "Number of iterations: 176  loss: 0.0034062834\n",
      "Number of iterations: 177  loss: 0.0033812984\n",
      "Number of iterations: 178  loss: 0.0033567091\n",
      "Number of iterations: 179  loss: 0.0033325062\n",
      "Number of iterations: 180  loss: 0.0033086764\n",
      "Number of iterations: 181  loss: 0.0032852148\n",
      "Number of iterations: 182  loss: 0.0032621056\n",
      "Number of iterations: 183  loss: 0.003239344\n",
      "Number of iterations: 184  loss: 0.003216917\n",
      "Number of iterations: 185  loss: 0.003194818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 186  loss: 0.0031730377\n",
      "Number of iterations: 187  loss: 0.0031515646\n",
      "Number of iterations: 188  loss: 0.0031303957\n",
      "Number of iterations: 189  loss: 0.0031095194\n",
      "Number of iterations: 190  loss: 0.0030889292\n",
      "Number of iterations: 191  loss: 0.0030686145\n",
      "Number of iterations: 192  loss: 0.0030485706\n",
      "Number of iterations: 193  loss: 0.0030287893\n",
      "Number of iterations: 194  loss: 0.0030092665\n",
      "Number of iterations: 195  loss: 0.0029899906\n",
      "Number of iterations: 196  loss: 0.0029709607\n",
      "Number of iterations: 197  loss: 0.0029521643\n",
      "Number of iterations: 198  loss: 0.0029336005\n",
      "Number of iterations: 199  loss: 0.0029152604\n",
      "Number of iterations: 200  loss: 0.0028971394\n",
      "Number of iterations: 201  loss: 0.0028792322\n",
      "Number of iterations: 202  loss: 0.0028615368\n",
      "Number of iterations: 203  loss: 0.0028440417\n",
      "Number of iterations: 204  loss: 0.0028267486\n",
      "Number of iterations: 205  loss: 0.0028096458\n",
      "Number of iterations: 206  loss: 0.0027927356\n",
      "Number of iterations: 207  loss: 0.002776008\n",
      "Number of iterations: 208  loss: 0.0027594639\n",
      "Number of iterations: 209  loss: 0.0027430966\n",
      "Number of iterations: 210  loss: 0.0027269048\n",
      "Number of iterations: 211  loss: 0.002710881\n",
      "Number of iterations: 212  loss: 0.0026950259\n",
      "Number of iterations: 213  loss: 0.0026793315\n",
      "Number of iterations: 214  loss: 0.0026637984\n",
      "Number of iterations: 215  loss: 0.0026484195\n",
      "Number of iterations: 216  loss: 0.0026331954\n",
      "Number of iterations: 217  loss: 0.0026181221\n",
      "Number of iterations: 218  loss: 0.0026031986\n",
      "Number of iterations: 219  loss: 0.002588416\n",
      "Number of iterations: 220  loss: 0.0025737805\n",
      "Number of iterations: 221  loss: 0.0025592837\n",
      "Number of iterations: 222  loss: 0.0025449258\n",
      "Number of iterations: 223  loss: 0.0025307012\n",
      "Number of iterations: 224  loss: 0.0025166143\n",
      "Number of iterations: 225  loss: 0.0025026533\n",
      "Number of iterations: 226  loss: 0.0024888301\n",
      "Number of iterations: 227  loss: 0.002475123\n",
      "Number of iterations: 228  loss: 0.002461556\n",
      "Number of iterations: 229  loss: 0.0024480934\n",
      "Number of iterations: 230  loss: 0.002434783\n",
      "Number of iterations: 231  loss: 0.0024215556\n",
      "Number of iterations: 232  loss: 0.0024084998\n",
      "Number of iterations: 233  loss: 0.002395486\n",
      "Number of iterations: 234  loss: 0.0023827003\n",
      "Number of iterations: 235  loss: 0.0023698676\n",
      "Number of iterations: 236  loss: 0.0023573863\n",
      "Number of iterations: 237  loss: 0.0023446714\n",
      "Number of iterations: 238  loss: 0.0023325968\n",
      "Number of iterations: 239  loss: 0.0023198647\n",
      "Number of iterations: 240  loss: 0.0023084674\n",
      "Number of iterations: 241  loss: 0.002295451\n",
      "Number of iterations: 242  loss: 0.0022855725\n",
      "Number of iterations: 243  loss: 0.0022719447\n",
      "Number of iterations: 244  loss: 0.0022668876\n",
      "Number of iterations: 245  loss: 0.0022537012\n",
      "Number of iterations: 246  loss: 0.002271428\n",
      "Number of iterations: 247  loss: 0.002270265\n",
      "Number of iterations: 248  loss: 0.0023924639\n",
      "Number of iterations: 249  loss: 0.0023713002\n",
      "Number of iterations: 250  loss: 0.0024293654\n",
      "Number of iterations: 251  loss: 0.0022557455\n",
      "Number of iterations: 252  loss: 0.00218754\n",
      "Number of iterations: 253  loss: 0.0021730352\n",
      "Number of iterations: 254  loss: 0.002165891\n",
      "Number of iterations: 255  loss: 0.0021635233\n",
      "Number of iterations: 256  loss: 0.0021332742\n",
      "Number of iterations: 257  loss: 0.0021221465\n",
      "Number of iterations: 258  loss: 0.0021122987\n",
      "Number of iterations: 259  loss: 0.0020996786\n",
      "Number of iterations: 260  loss: 0.0020928031\n",
      "Number of iterations: 261  loss: 0.0020793108\n",
      "Number of iterations: 262  loss: 0.0020703\n",
      "Number of iterations: 263  loss: 0.0020603666\n",
      "Number of iterations: 264  loss: 0.0020499993\n",
      "Number of iterations: 265  loss: 0.002041222\n",
      "Number of iterations: 266  loss: 0.0020307975\n",
      "Number of iterations: 267  loss: 0.002021724\n",
      "Number of iterations: 268  loss: 0.0020120575\n",
      "Number of iterations: 269  loss: 0.0020026364\n",
      "Number of iterations: 270  loss: 0.0019934943\n",
      "Number of iterations: 271  loss: 0.001984057\n",
      "Number of iterations: 272  loss: 0.001975063\n",
      "Number of iterations: 273  loss: 0.001965839\n",
      "Number of iterations: 274  loss: 0.0019568705\n",
      "Number of iterations: 275  loss: 0.0019478864\n",
      "Number of iterations: 276  loss: 0.0019389737\n",
      "Number of iterations: 277  loss: 0.0019301679\n",
      "Number of iterations: 278  loss: 0.0019213605\n",
      "Number of iterations: 279  loss: 0.0019126821\n",
      "Number of iterations: 280  loss: 0.001904009\n",
      "Number of iterations: 281  loss: 0.0018954409\n",
      "Number of iterations: 282  loss: 0.0018869037\n",
      "Number of iterations: 283  loss: 0.0018784442\n",
      "Number of iterations: 284  loss: 0.0018700364\n",
      "Number of iterations: 285  loss: 0.0018616883\n",
      "Number of iterations: 286  loss: 0.0018534019\n",
      "Number of iterations: 287  loss: 0.0018451678\n",
      "Number of iterations: 288  loss: 0.0018369977\n",
      "Number of iterations: 289  loss: 0.001828878\n",
      "Number of iterations: 290  loss: 0.0018208191\n",
      "Number of iterations: 291  loss: 0.0018128097\n",
      "Number of iterations: 292  loss: 0.0018048596\n",
      "Number of iterations: 293  loss: 0.0017969593\n",
      "Number of iterations: 294  loss: 0.0017891151\n",
      "Number of iterations: 295  loss: 0.0017813229\n",
      "Number of iterations: 296  loss: 0.0017735822\n",
      "Number of iterations: 297  loss: 0.0017658937\n",
      "Number of iterations: 298  loss: 0.0017582562\n",
      "Number of iterations: 299  loss: 0.0017506678\n",
      "Number of iterations: 300  loss: 0.0017431306\n",
      "Number of iterations: 301  loss: 0.0017356406\n",
      "Number of iterations: 302  loss: 0.0017282016\n",
      "Number of iterations: 303  loss: 0.0017208088\n",
      "Number of iterations: 304  loss: 0.001713465\n",
      "Number of iterations: 305  loss: 0.0017061661\n",
      "Number of iterations: 306  loss: 0.0016989175\n",
      "Number of iterations: 307  loss: 0.0016917106\n",
      "Number of iterations: 308  loss: 0.001684555\n",
      "Number of iterations: 309  loss: 0.0016774371\n",
      "Number of iterations: 310  loss: 0.0016703727\n",
      "Number of iterations: 311  loss: 0.0016633448\n",
      "Number of iterations: 312  loss: 0.001656369\n",
      "Number of iterations: 313  loss: 0.0016494266\n",
      "Number of iterations: 314  loss: 0.0016425409\n",
      "Number of iterations: 315  loss: 0.0016356792\n",
      "Number of iterations: 316  loss: 0.0016288852\n",
      "Number of iterations: 317  loss: 0.0016220994\n",
      "Number of iterations: 318  loss: 0.001615403\n",
      "Number of iterations: 319  loss: 0.0016086833\n",
      "Number of iterations: 320  loss: 0.0016020924\n",
      "Number of iterations: 321  loss: 0.0015954213\n",
      "Number of iterations: 322  loss: 0.0015889602\n",
      "Number of iterations: 323  loss: 0.0015823013\n",
      "Number of iterations: 324  loss: 0.0015760269\n",
      "Number of iterations: 325  loss: 0.001569305\n",
      "Number of iterations: 326  loss: 0.00156335\n",
      "Number of iterations: 327  loss: 0.0015564149\n",
      "Number of iterations: 328  loss: 0.001551142\n",
      "Number of iterations: 329  loss: 0.0015437434\n",
      "Number of iterations: 330  loss: 0.0015402837\n",
      "Number of iterations: 331  loss: 0.0015324058\n",
      "Number of iterations: 332  loss: 0.0015353477\n",
      "Number of iterations: 333  loss: 0.0015303431\n",
      "Number of iterations: 334  loss: 0.0015645821\n",
      "Number of iterations: 335  loss: 0.0015890873\n",
      "Number of iterations: 336  loss: 0.0018070464\n",
      "Number of iterations: 337  loss: 0.001923416\n",
      "Number of iterations: 338  loss: 0.0023775687\n",
      "Number of iterations: 339  loss: 0.0020009791\n",
      "Number of iterations: 340  loss: 0.0016488576\n",
      "Number of iterations: 341  loss: 0.0014838693\n",
      "Number of iterations: 342  loss: 0.0015130951\n",
      "Number of iterations: 343  loss: 0.0015386743\n",
      "Number of iterations: 344  loss: 0.0014836023\n",
      "Number of iterations: 345  loss: 0.0014591961\n",
      "Number of iterations: 346  loss: 0.0014522519\n",
      "Number of iterations: 347  loss: 0.0014450768\n",
      "Number of iterations: 348  loss: 0.0014441111\n",
      "Number of iterations: 349  loss: 0.0014307392\n",
      "Number of iterations: 350  loss: 0.0014259686\n",
      "Number of iterations: 351  loss: 0.0014214686\n",
      "Number of iterations: 352  loss: 0.0014149513\n",
      "Number of iterations: 353  loss: 0.0014111969\n",
      "Number of iterations: 354  loss: 0.0014046371\n",
      "Number of iterations: 355  loss: 0.0013998543\n",
      "Number of iterations: 356  loss: 0.0013947473\n",
      "Number of iterations: 357  loss: 0.0013892627\n",
      "Number of iterations: 358  loss: 0.0013845464\n",
      "Number of iterations: 359  loss: 0.0013790764\n",
      "Number of iterations: 360  loss: 0.0013741913\n",
      "Number of iterations: 361  loss: 0.0013690549\n",
      "Number of iterations: 362  loss: 0.0013639918\n",
      "Number of iterations: 363  loss: 0.0013590618\n",
      "Number of iterations: 364  loss: 0.0013539899\n",
      "Number of iterations: 365  loss: 0.0013491076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 366  loss: 0.0013441207\n",
      "Number of iterations: 367  loss: 0.001339236\n",
      "Number of iterations: 368  loss: 0.0013343428\n",
      "Number of iterations: 369  loss: 0.0013294771\n",
      "Number of iterations: 370  loss: 0.001324653\n",
      "Number of iterations: 371  loss: 0.0013198297\n",
      "Number of iterations: 372  loss: 0.0013150587\n",
      "Number of iterations: 373  loss: 0.0013102912\n",
      "Number of iterations: 374  loss: 0.001305568\n",
      "Number of iterations: 375  loss: 0.0013008591\n",
      "Number of iterations: 376  loss: 0.0012961845\n",
      "Number of iterations: 377  loss: 0.001291533\n",
      "Number of iterations: 378  loss: 0.0012869091\n",
      "Number of iterations: 379  loss: 0.0012823131\n",
      "Number of iterations: 380  loss: 0.001277741\n",
      "Number of iterations: 381  loss: 0.0012731993\n",
      "Number of iterations: 382  loss: 0.0012686818\n",
      "Number of iterations: 383  loss: 0.0012641931\n",
      "Number of iterations: 384  loss: 0.0012597267\n",
      "Number of iterations: 385  loss: 0.0012552913\n",
      "Number of iterations: 386  loss: 0.0012508796\n",
      "Number of iterations: 387  loss: 0.0012464972\n",
      "Number of iterations: 388  loss: 0.0012421369\n",
      "Number of iterations: 389  loss: 0.0012378057\n",
      "Number of iterations: 390  loss: 0.0012334976\n",
      "Number of iterations: 391  loss: 0.0012292174\n",
      "Number of iterations: 392  loss: 0.001224961\n",
      "Number of iterations: 393  loss: 0.0012207322\n",
      "Number of iterations: 394  loss: 0.0012165279\n",
      "Number of iterations: 395  loss: 0.0012123494\n",
      "Number of iterations: 396  loss: 0.0012081948\n",
      "Number of iterations: 397  loss: 0.0012040663\n",
      "Number of iterations: 398  loss: 0.0011999621\n",
      "Number of iterations: 399  loss: 0.0011958835\n",
      "Number of iterations: 400  loss: 0.0011918275\n",
      "Number of iterations: 401  loss: 0.0011877985\n",
      "Number of iterations: 402  loss: 0.0011837909\n",
      "Number of iterations: 403  loss: 0.001179811\n",
      "Number of iterations: 404  loss: 0.0011758519\n",
      "Number of iterations: 405  loss: 0.0011719203\n",
      "Number of iterations: 406  loss: 0.0011680077\n",
      "Number of iterations: 407  loss: 0.0011641252\n",
      "Number of iterations: 408  loss: 0.0011602583\n",
      "Number of iterations: 409  loss: 0.0011564259\n",
      "Number of iterations: 410  loss: 0.0011526013\n",
      "Number of iterations: 411  loss: 0.0011488211\n",
      "Number of iterations: 412  loss: 0.0011450339\n",
      "Number of iterations: 413  loss: 0.0011413129\n",
      "Number of iterations: 414  loss: 0.0011375542\n",
      "Number of iterations: 415  loss: 0.0011339035\n",
      "Number of iterations: 416  loss: 0.0011301548\n",
      "Number of iterations: 417  loss: 0.001126609\n",
      "Number of iterations: 418  loss: 0.0011228273\n",
      "Number of iterations: 419  loss: 0.0011194682\n",
      "Number of iterations: 420  loss: 0.0011155736\n",
      "Number of iterations: 421  loss: 0.0011126422\n",
      "Number of iterations: 422  loss: 0.0011085401\n",
      "Number of iterations: 423  loss: 0.0011068995\n",
      "Number of iterations: 424  loss: 0.0011029828\n",
      "Number of iterations: 425  loss: 0.0011066687\n",
      "Number of iterations: 426  loss: 0.0011079857\n",
      "Number of iterations: 427  loss: 0.0011417522\n",
      "Number of iterations: 428  loss: 0.0011864948\n",
      "Number of iterations: 429  loss: 0.0014351499\n",
      "Number of iterations: 430  loss: 0.0017153635\n",
      "Number of iterations: 431  loss: 0.0027812961\n",
      "Number of iterations: 432  loss: 0.0025325986\n",
      "Number of iterations: 433  loss: 0.0018125427\n",
      "Number of iterations: 434  loss: 0.0011310497\n",
      "Number of iterations: 435  loss: 0.0011656053\n",
      "Number of iterations: 436  loss: 0.0012539343\n",
      "Number of iterations: 437  loss: 0.0011416232\n",
      "Number of iterations: 438  loss: 0.0010685539\n",
      "Number of iterations: 439  loss: 0.0010734398\n",
      "Number of iterations: 440  loss: 0.0010726864\n",
      "Number of iterations: 441  loss: 0.001065547\n",
      "Number of iterations: 442  loss: 0.0010485464\n",
      "Number of iterations: 443  loss: 0.001045732\n",
      "Number of iterations: 444  loss: 0.0010466897\n",
      "Number of iterations: 445  loss: 0.0010409191\n",
      "Number of iterations: 446  loss: 0.0010383571\n",
      "Number of iterations: 447  loss: 0.00103481\n",
      "Number of iterations: 448  loss: 0.0010318777\n",
      "Number of iterations: 449  loss: 0.0010297386\n",
      "Number of iterations: 450  loss: 0.0010262198\n",
      "Number of iterations: 451  loss: 0.0010236192\n",
      "Number of iterations: 452  loss: 0.0010206756\n",
      "Number of iterations: 453  loss: 0.0010177493\n",
      "Number of iterations: 454  loss: 0.0010150924\n",
      "Number of iterations: 455  loss: 0.0010121046\n",
      "Number of iterations: 456  loss: 0.0010093933\n",
      "Number of iterations: 457  loss: 0.0010065557\n",
      "Number of iterations: 458  loss: 0.0010037593\n",
      "Number of iterations: 459  loss: 0.001001016\n",
      "Number of iterations: 460  loss: 0.0009982059\n",
      "Number of iterations: 461  loss: 0.0009954767\n",
      "Number of iterations: 462  loss: 0.0009927062\n",
      "Number of iterations: 463  loss: 0.0009899738\n",
      "Number of iterations: 464  loss: 0.0009872459\n",
      "Number of iterations: 465  loss: 0.000984523\n",
      "Number of iterations: 466  loss: 0.0009818277\n",
      "Number of iterations: 467  loss: 0.0009791299\n",
      "Number of iterations: 468  loss: 0.00097646\n",
      "Number of iterations: 469  loss: 0.00097379496\n",
      "Number of iterations: 470  loss: 0.0009711499\n",
      "Number of iterations: 471  loss: 0.0009685168\n",
      "Number of iterations: 472  loss: 0.0009658992\n",
      "Number of iterations: 473  loss: 0.0009632974\n",
      "Number of iterations: 474  loss: 0.000960709\n",
      "Number of iterations: 475  loss: 0.0009581374\n",
      "Number of iterations: 476  loss: 0.0009555787\n",
      "Number of iterations: 477  loss: 0.00095303665\n",
      "Number of iterations: 478  loss: 0.0009505088\n",
      "Number of iterations: 479  loss: 0.0009479959\n",
      "Number of iterations: 480  loss: 0.0009454981\n",
      "Number of iterations: 481  loss: 0.0009430157\n",
      "Number of iterations: 482  loss: 0.00094054773\n",
      "Number of iterations: 483  loss: 0.00093809416\n",
      "Number of iterations: 484  loss: 0.00093565596\n",
      "Number of iterations: 485  loss: 0.0009332318\n",
      "Number of iterations: 486  loss: 0.000930823\n",
      "Number of iterations: 487  loss: 0.0009284278\n",
      "Number of iterations: 488  loss: 0.00092604745\n",
      "Number of iterations: 489  loss: 0.00092368165\n",
      "Number of iterations: 490  loss: 0.0009213295\n",
      "Number of iterations: 491  loss: 0.0009189926\n",
      "Number of iterations: 492  loss: 0.000916668\n",
      "Number of iterations: 493  loss: 0.00091435836\n",
      "Number of iterations: 494  loss: 0.00091206253\n",
      "Number of iterations: 495  loss: 0.0009097806\n",
      "Number of iterations: 496  loss: 0.000907512\n",
      "Number of iterations: 497  loss: 0.0009052568\n",
      "Number of iterations: 498  loss: 0.0009030156\n",
      "Number of iterations: 499  loss: 0.00090078806\n",
      "Number of iterations: 500  loss: 0.0008985725\n",
      "Number of iterations: 501  loss: 0.0008963719\n",
      "Number of iterations: 502  loss: 0.0008941813\n",
      "Number of iterations: 503  loss: 0.0008920081\n",
      "Number of iterations: 504  loss: 0.00088984345\n",
      "Number of iterations: 505  loss: 0.00088769733\n",
      "Number of iterations: 506  loss: 0.0008855553\n",
      "Number of iterations: 507  loss: 0.00088343775\n",
      "Number of iterations: 508  loss: 0.00088131736\n",
      "Number of iterations: 509  loss: 0.00087922905\n",
      "Number of iterations: 510  loss: 0.00087712787\n",
      "Number of iterations: 511  loss: 0.00087507354\n",
      "Number of iterations: 512  loss: 0.00087298435\n",
      "Number of iterations: 513  loss: 0.0008709729\n",
      "Number of iterations: 514  loss: 0.0008688834\n",
      "Number of iterations: 515  loss: 0.0008669383\n",
      "Number of iterations: 516  loss: 0.00086482504\n",
      "Number of iterations: 517  loss: 0.00086300157\n",
      "Number of iterations: 518  loss: 0.0008608298\n",
      "Number of iterations: 519  loss: 0.0008592948\n",
      "Number of iterations: 520  loss: 0.0008570719\n",
      "Number of iterations: 521  loss: 0.0008564312\n",
      "Number of iterations: 522  loss: 0.000854664\n",
      "Number of iterations: 523  loss: 0.0008576745\n",
      "Number of iterations: 524  loss: 0.00086044514\n",
      "Number of iterations: 525  loss: 0.0008825611\n",
      "Number of iterations: 526  loss: 0.0009163623\n",
      "Number of iterations: 527  loss: 0.0010595653\n",
      "Number of iterations: 528  loss: 0.0012696562\n",
      "Number of iterations: 529  loss: 0.002106779\n",
      "Number of iterations: 530  loss: 0.0025882137\n",
      "Number of iterations: 531  loss: 0.0031550238\n",
      "Number of iterations: 532  loss: 0.0018311974\n",
      "Number of iterations: 533  loss: 0.0008848902\n",
      "Number of iterations: 534  loss: 0.0009581964\n",
      "Number of iterations: 535  loss: 0.001058686\n",
      "Number of iterations: 536  loss: 0.00094152463\n",
      "Number of iterations: 537  loss: 0.0008400747\n",
      "Number of iterations: 538  loss: 0.0008406606\n",
      "Number of iterations: 539  loss: 0.00085967383\n",
      "Number of iterations: 540  loss: 0.0008393404\n",
      "Number of iterations: 541  loss: 0.00082855194\n",
      "Number of iterations: 542  loss: 0.0008246238\n",
      "Number of iterations: 543  loss: 0.00082372694\n",
      "Number of iterations: 544  loss: 0.00082338275\n",
      "Number of iterations: 545  loss: 0.0008191548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 546  loss: 0.0008173634\n",
      "Number of iterations: 547  loss: 0.0008159087\n",
      "Number of iterations: 548  loss: 0.00081404124\n",
      "Number of iterations: 549  loss: 0.00081271527\n",
      "Number of iterations: 550  loss: 0.00081067235\n",
      "Number of iterations: 551  loss: 0.00080910575\n",
      "Number of iterations: 552  loss: 0.0008074872\n",
      "Number of iterations: 553  loss: 0.00080578675\n",
      "Number of iterations: 554  loss: 0.0008042793\n",
      "Number of iterations: 555  loss: 0.00080258166\n",
      "Number of iterations: 556  loss: 0.0008010249\n",
      "Number of iterations: 557  loss: 0.0007994102\n",
      "Number of iterations: 558  loss: 0.0007978046\n",
      "Number of iterations: 559  loss: 0.0007962316\n",
      "Number of iterations: 560  loss: 0.00079461804\n",
      "Number of iterations: 561  loss: 0.0007930483\n",
      "Number of iterations: 562  loss: 0.0007914535\n",
      "Number of iterations: 563  loss: 0.0007898826\n",
      "Number of iterations: 564  loss: 0.0007883133\n",
      "Number of iterations: 565  loss: 0.0007867491\n",
      "Number of iterations: 566  loss: 0.0007851987\n",
      "Number of iterations: 567  loss: 0.0007836491\n",
      "Number of iterations: 568  loss: 0.0007821151\n",
      "Number of iterations: 569  loss: 0.0007805841\n",
      "Number of iterations: 570  loss: 0.0007790653\n",
      "Number of iterations: 571  loss: 0.00077755225\n",
      "Number of iterations: 572  loss: 0.0007760499\n",
      "Number of iterations: 573  loss: 0.00077455555\n",
      "Number of iterations: 574  loss: 0.0007730694\n",
      "Number of iterations: 575  loss: 0.000771592\n",
      "Number of iterations: 576  loss: 0.0007701235\n",
      "Number of iterations: 577  loss: 0.00076866354\n",
      "Number of iterations: 578  loss: 0.0007672116\n",
      "Number of iterations: 579  loss: 0.00076576875\n",
      "Number of iterations: 580  loss: 0.00076433463\n",
      "Number of iterations: 581  loss: 0.0007629082\n",
      "Number of iterations: 582  loss: 0.00076149026\n",
      "Number of iterations: 583  loss: 0.00076008035\n",
      "Number of iterations: 584  loss: 0.0007586791\n",
      "Number of iterations: 585  loss: 0.0007572867\n",
      "Number of iterations: 586  loss: 0.00075590034\n",
      "Number of iterations: 587  loss: 0.0007545245\n",
      "Number of iterations: 588  loss: 0.00075315486\n",
      "Number of iterations: 589  loss: 0.0007517941\n",
      "Number of iterations: 590  loss: 0.0007504405\n",
      "Number of iterations: 591  loss: 0.0007490947\n",
      "Number of iterations: 592  loss: 0.00074775674\n",
      "Number of iterations: 593  loss: 0.0007464268\n",
      "Number of iterations: 594  loss: 0.00074510387\n",
      "Number of iterations: 595  loss: 0.0007437885\n",
      "Number of iterations: 596  loss: 0.0007424796\n",
      "Number of iterations: 597  loss: 0.0007411799\n",
      "Number of iterations: 598  loss: 0.0007398841\n",
      "Number of iterations: 599  loss: 0.0007385994\n",
      "Number of iterations: 600  loss: 0.0007373178\n",
      "Number of iterations: 601  loss: 0.00073604804\n",
      "Number of iterations: 602  loss: 0.0007347796\n",
      "Number of iterations: 603  loss: 0.0007335229\n",
      "Number of iterations: 604  loss: 0.000732268\n",
      "Number of iterations: 605  loss: 0.0007310271\n",
      "Number of iterations: 606  loss: 0.0007297828\n",
      "Number of iterations: 607  loss: 0.00072855904\n",
      "Number of iterations: 608  loss: 0.00072732306\n",
      "Number of iterations: 609  loss: 0.00072611874\n",
      "Number of iterations: 610  loss: 0.0007248876\n",
      "Number of iterations: 611  loss: 0.0007237091\n",
      "Number of iterations: 612  loss: 0.0007224745\n",
      "Number of iterations: 613  loss: 0.0007213386\n",
      "Number of iterations: 614  loss: 0.00072008785\n",
      "Number of iterations: 615  loss: 0.0007190318\n",
      "Number of iterations: 616  loss: 0.000717752\n",
      "Number of iterations: 617  loss: 0.0007168992\n",
      "Number of iterations: 618  loss: 0.0007156356\n",
      "Number of iterations: 619  loss: 0.0007154368\n",
      "Number of iterations: 620  loss: 0.00071468897\n",
      "Number of iterations: 621  loss: 0.0007172053\n",
      "Number of iterations: 622  loss: 0.0007203598\n",
      "Number of iterations: 623  loss: 0.00073671434\n",
      "Number of iterations: 624  loss: 0.00076437346\n",
      "Number of iterations: 625  loss: 0.00086347514\n",
      "Number of iterations: 626  loss: 0.0010300912\n",
      "Number of iterations: 627  loss: 0.0016409132\n",
      "Number of iterations: 628  loss: 0.0023037882\n",
      "Number of iterations: 629  loss: 0.003831505\n",
      "Number of iterations: 630  loss: 0.0030982425\n",
      "Number of iterations: 631  loss: 0.0014512875\n",
      "Number of iterations: 632  loss: 0.0007272208\n",
      "Number of iterations: 633  loss: 0.0009165064\n",
      "Number of iterations: 634  loss: 0.0009727374\n",
      "Number of iterations: 635  loss: 0.00079776446\n",
      "Number of iterations: 636  loss: 0.0007115546\n",
      "Number of iterations: 637  loss: 0.00072490925\n",
      "Number of iterations: 638  loss: 0.0007303009\n",
      "Number of iterations: 639  loss: 0.0007197193\n",
      "Number of iterations: 640  loss: 0.00070212485\n",
      "Number of iterations: 641  loss: 0.0007001878\n",
      "Number of iterations: 642  loss: 0.00070157385\n",
      "Number of iterations: 643  loss: 0.0006988721\n",
      "Number of iterations: 644  loss: 0.00069722935\n",
      "Number of iterations: 645  loss: 0.000694945\n",
      "Number of iterations: 646  loss: 0.0006938331\n",
      "Number of iterations: 647  loss: 0.00069310406\n",
      "Number of iterations: 648  loss: 0.0006916746\n",
      "Number of iterations: 649  loss: 0.0006907054\n",
      "Number of iterations: 650  loss: 0.0006894863\n",
      "Number of iterations: 651  loss: 0.0006884459\n",
      "Number of iterations: 652  loss: 0.0006874893\n",
      "Number of iterations: 653  loss: 0.0006864009\n",
      "Number of iterations: 654  loss: 0.00068544416\n",
      "Number of iterations: 655  loss: 0.0006843913\n",
      "Number of iterations: 656  loss: 0.00068339956\n",
      "Number of iterations: 657  loss: 0.00068240095\n",
      "Number of iterations: 658  loss: 0.000681392\n",
      "Number of iterations: 659  loss: 0.00068041176\n",
      "Number of iterations: 660  loss: 0.00067940867\n",
      "Number of iterations: 661  loss: 0.00067843293\n",
      "Number of iterations: 662  loss: 0.00067744945\n",
      "Number of iterations: 663  loss: 0.0006764772\n",
      "Number of iterations: 664  loss: 0.0006755116\n",
      "Number of iterations: 665  loss: 0.0006745486\n",
      "Number of iterations: 666  loss: 0.0006735972\n",
      "Number of iterations: 667  loss: 0.00067264726\n",
      "Number of iterations: 668  loss: 0.00067170773\n",
      "Number of iterations: 669  loss: 0.00067077175\n",
      "Number of iterations: 670  loss: 0.000669844\n",
      "Number of iterations: 671  loss: 0.0006689206\n",
      "Number of iterations: 672  loss: 0.0006680044\n",
      "Number of iterations: 673  loss: 0.00066709414\n",
      "Number of iterations: 674  loss: 0.00066618965\n",
      "Number of iterations: 675  loss: 0.00066529197\n",
      "Number of iterations: 676  loss: 0.0006643991\n",
      "Number of iterations: 677  loss: 0.0006635124\n",
      "Number of iterations: 678  loss: 0.0006626319\n",
      "Number of iterations: 679  loss: 0.0006617574\n",
      "Number of iterations: 680  loss: 0.00066088774\n",
      "Number of iterations: 681  loss: 0.00066002423\n",
      "Number of iterations: 682  loss: 0.00065916596\n",
      "Number of iterations: 683  loss: 0.00065831374\n",
      "Number of iterations: 684  loss: 0.0006574664\n",
      "Number of iterations: 685  loss: 0.00065662444\n",
      "Number of iterations: 686  loss: 0.00065578724\n",
      "Number of iterations: 687  loss: 0.00065495545\n",
      "Number of iterations: 688  loss: 0.00065412855\n",
      "Number of iterations: 689  loss: 0.00065330695\n",
      "Number of iterations: 690  loss: 0.000652489\n",
      "Number of iterations: 691  loss: 0.0006516776\n",
      "Number of iterations: 692  loss: 0.00065086916\n",
      "Number of iterations: 693  loss: 0.0006500673\n",
      "Number of iterations: 694  loss: 0.00064926763\n",
      "Number of iterations: 695  loss: 0.00064847455\n",
      "Number of iterations: 696  loss: 0.0006476834\n",
      "Number of iterations: 697  loss: 0.00064689893\n",
      "Number of iterations: 698  loss: 0.00064611615\n",
      "Number of iterations: 699  loss: 0.00064534077\n",
      "Number of iterations: 700  loss: 0.00064456556\n",
      "Number of iterations: 701  loss: 0.00064379897\n",
      "Number of iterations: 702  loss: 0.00064303\n",
      "Number of iterations: 703  loss: 0.0006422731\n",
      "Number of iterations: 704  loss: 0.00064150995\n",
      "Number of iterations: 705  loss: 0.0006407633\n",
      "Number of iterations: 706  loss: 0.00064000406\n",
      "Number of iterations: 707  loss: 0.00063927076\n",
      "Number of iterations: 708  loss: 0.00063851115\n",
      "Number of iterations: 709  loss: 0.0006377964\n",
      "Number of iterations: 710  loss: 0.0006370318\n",
      "Number of iterations: 711  loss: 0.0006363489\n",
      "Number of iterations: 712  loss: 0.0006355711\n",
      "Number of iterations: 713  loss: 0.0006349543\n",
      "Number of iterations: 714  loss: 0.00063416344\n",
      "Number of iterations: 715  loss: 0.00063372124\n",
      "Number of iterations: 716  loss: 0.0006329888\n",
      "Number of iterations: 717  loss: 0.00063312927\n",
      "Number of iterations: 718  loss: 0.0006329744\n",
      "Number of iterations: 719  loss: 0.000635497\n",
      "Number of iterations: 720  loss: 0.0006390246\n",
      "Number of iterations: 721  loss: 0.00065311155\n",
      "Number of iterations: 722  loss: 0.00067777047\n",
      "Number of iterations: 723  loss: 0.0007558827\n",
      "Number of iterations: 724  loss: 0.00089501747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 725  loss: 0.0013499528\n",
      "Number of iterations: 726  loss: 0.001975503\n",
      "Number of iterations: 727  loss: 0.0036698538\n",
      "Number of iterations: 728  loss: 0.003903454\n",
      "Number of iterations: 729  loss: 0.0028623417\n",
      "Number of iterations: 730  loss: 0.0011064603\n",
      "Number of iterations: 731  loss: 0.0006743642\n",
      "Number of iterations: 732  loss: 0.00092941476\n",
      "Number of iterations: 733  loss: 0.0008657766\n",
      "Number of iterations: 734  loss: 0.00069019065\n",
      "Number of iterations: 735  loss: 0.00063622394\n",
      "Number of iterations: 736  loss: 0.0006547888\n",
      "Number of iterations: 737  loss: 0.00066076155\n",
      "Number of iterations: 738  loss: 0.0006375708\n",
      "Number of iterations: 739  loss: 0.0006296733\n",
      "Number of iterations: 740  loss: 0.000628824\n",
      "Number of iterations: 741  loss: 0.00062802114\n",
      "Number of iterations: 742  loss: 0.0006271112\n",
      "Number of iterations: 743  loss: 0.0006248312\n",
      "Number of iterations: 744  loss: 0.00062382565\n",
      "Number of iterations: 745  loss: 0.0006229565\n",
      "Number of iterations: 746  loss: 0.0006220784\n",
      "Number of iterations: 747  loss: 0.000621387\n",
      "Number of iterations: 748  loss: 0.0006204536\n",
      "Number of iterations: 749  loss: 0.00061974715\n",
      "Number of iterations: 750  loss: 0.0006189501\n",
      "Number of iterations: 751  loss: 0.0006182169\n",
      "Number of iterations: 752  loss: 0.000617499\n",
      "Number of iterations: 753  loss: 0.00061674486\n",
      "Number of iterations: 754  loss: 0.00061604806\n",
      "Number of iterations: 755  loss: 0.00061530597\n",
      "Number of iterations: 756  loss: 0.00061460555\n",
      "Number of iterations: 757  loss: 0.0006138895\n",
      "Number of iterations: 758  loss: 0.00061318517\n",
      "Number of iterations: 759  loss: 0.00061249075\n",
      "Number of iterations: 760  loss: 0.0006117903\n",
      "Number of iterations: 761  loss: 0.0006111097\n",
      "Number of iterations: 762  loss: 0.00061042263\n",
      "Number of iterations: 763  loss: 0.0006097492\n",
      "Number of iterations: 764  loss: 0.0006090762\n",
      "Number of iterations: 765  loss: 0.00060840964\n",
      "Number of iterations: 766  loss: 0.00060774974\n",
      "Number of iterations: 767  loss: 0.00060709164\n",
      "Number of iterations: 768  loss: 0.000606441\n",
      "Number of iterations: 769  loss: 0.00060579326\n",
      "Number of iterations: 770  loss: 0.00060515146\n",
      "Number of iterations: 771  loss: 0.0006045135\n",
      "Number of iterations: 772  loss: 0.00060387875\n",
      "Number of iterations: 773  loss: 0.00060324883\n",
      "Number of iterations: 774  loss: 0.0006026239\n",
      "Number of iterations: 775  loss: 0.0006020024\n",
      "Number of iterations: 776  loss: 0.0006013849\n",
      "Number of iterations: 777  loss: 0.00060077116\n",
      "Number of iterations: 778  loss: 0.0006001616\n",
      "Number of iterations: 779  loss: 0.0005995548\n",
      "Number of iterations: 780  loss: 0.0005989525\n",
      "Number of iterations: 781  loss: 0.0005983532\n",
      "Number of iterations: 782  loss: 0.0005977579\n",
      "Number of iterations: 783  loss: 0.0005971645\n",
      "Number of iterations: 784  loss: 0.0005965756\n",
      "Number of iterations: 785  loss: 0.00059598876\n",
      "Number of iterations: 786  loss: 0.0005954063\n",
      "Number of iterations: 787  loss: 0.00059482513\n",
      "Number of iterations: 788  loss: 0.00059424876\n",
      "Number of iterations: 789  loss: 0.0005936737\n",
      "Number of iterations: 790  loss: 0.00059310277\n",
      "Number of iterations: 791  loss: 0.00059253245\n",
      "Number of iterations: 792  loss: 0.0005919666\n",
      "Number of iterations: 793  loss: 0.0005914022\n",
      "Number of iterations: 794  loss: 0.00059084187\n",
      "Number of iterations: 795  loss: 0.00059028144\n",
      "Number of iterations: 796  loss: 0.00058972626\n",
      "Number of iterations: 797  loss: 0.0005891709\n",
      "Number of iterations: 798  loss: 0.00058862055\n",
      "Number of iterations: 799  loss: 0.0005880685\n",
      "Number of iterations: 800  loss: 0.0005875238\n",
      "Number of iterations: 801  loss: 0.00058697484\n",
      "Number of iterations: 802  loss: 0.00058643625\n",
      "Number of iterations: 803  loss: 0.0005858888\n",
      "Number of iterations: 804  loss: 0.00058535696\n",
      "Number of iterations: 805  loss: 0.00058480987\n",
      "Number of iterations: 806  loss: 0.0005842873\n",
      "Number of iterations: 807  loss: 0.00058373803\n",
      "Number of iterations: 808  loss: 0.00058323\n",
      "Number of iterations: 809  loss: 0.0005826719\n",
      "Number of iterations: 810  loss: 0.00058219186\n",
      "Number of iterations: 811  loss: 0.00058161706\n",
      "Number of iterations: 812  loss: 0.0005811981\n",
      "Number of iterations: 813  loss: 0.0005806022\n",
      "Number of iterations: 814  loss: 0.00058034516\n",
      "Number of iterations: 815  loss: 0.0005797863\n",
      "Number of iterations: 816  loss: 0.000580063\n",
      "Number of iterations: 817  loss: 0.0005799886\n",
      "Number of iterations: 818  loss: 0.0005824064\n",
      "Number of iterations: 819  loss: 0.0005855256\n",
      "Number of iterations: 820  loss: 0.000598098\n",
      "Number of iterations: 821  loss: 0.0006197959\n",
      "Number of iterations: 822  loss: 0.00068725576\n",
      "Number of iterations: 823  loss: 0.00081146165\n",
      "Number of iterations: 824  loss: 0.0011995153\n",
      "Number of iterations: 825  loss: 0.0018046692\n",
      "Number of iterations: 826  loss: 0.0035421848\n",
      "Number of iterations: 827  loss: 0.004411482\n",
      "Number of iterations: 828  loss: 0.0042997207\n",
      "Number of iterations: 829  loss: 0.0019188363\n",
      "Number of iterations: 830  loss: 0.0006024186\n",
      "Number of iterations: 831  loss: 0.0008229842\n",
      "Number of iterations: 832  loss: 0.00090785837\n",
      "Number of iterations: 833  loss: 0.000706232\n",
      "Number of iterations: 834  loss: 0.0005900883\n",
      "Number of iterations: 835  loss: 0.0006024796\n",
      "Number of iterations: 836  loss: 0.000619084\n",
      "Number of iterations: 837  loss: 0.0005938461\n",
      "Number of iterations: 838  loss: 0.0005823199\n",
      "Number of iterations: 839  loss: 0.00058120146\n",
      "Number of iterations: 840  loss: 0.00058090343\n",
      "Number of iterations: 841  loss: 0.00057971466\n",
      "Number of iterations: 842  loss: 0.0005774599\n",
      "Number of iterations: 843  loss: 0.0005765463\n",
      "Number of iterations: 844  loss: 0.000576005\n",
      "Number of iterations: 845  loss: 0.00057516986\n",
      "Number of iterations: 846  loss: 0.000574521\n",
      "Number of iterations: 847  loss: 0.00057379354\n",
      "Number of iterations: 848  loss: 0.0005731382\n",
      "Number of iterations: 849  loss: 0.0005725449\n",
      "Number of iterations: 850  loss: 0.00057190465\n",
      "Number of iterations: 851  loss: 0.0005713053\n",
      "Number of iterations: 852  loss: 0.00057071226\n",
      "Number of iterations: 853  loss: 0.0005701067\n",
      "Number of iterations: 854  loss: 0.0005695282\n",
      "Number of iterations: 855  loss: 0.00056894444\n",
      "Number of iterations: 856  loss: 0.00056836806\n",
      "Number of iterations: 857  loss: 0.0005678056\n",
      "Number of iterations: 858  loss: 0.00056723674\n",
      "Number of iterations: 859  loss: 0.00056668447\n",
      "Number of iterations: 860  loss: 0.000566133\n",
      "Number of iterations: 861  loss: 0.0005655873\n",
      "Number of iterations: 862  loss: 0.0005650501\n",
      "Number of iterations: 863  loss: 0.0005645137\n",
      "Number of iterations: 864  loss: 0.0005639877\n",
      "Number of iterations: 865  loss: 0.00056346226\n",
      "Number of iterations: 866  loss: 0.0005629444\n",
      "Number of iterations: 867  loss: 0.0005624298\n",
      "Number of iterations: 868  loss: 0.0005619199\n",
      "Number of iterations: 869  loss: 0.0005614146\n",
      "Number of iterations: 870  loss: 0.0005609136\n",
      "Number of iterations: 871  loss: 0.0005604168\n",
      "Number of iterations: 872  loss: 0.0005599237\n",
      "Number of iterations: 873  loss: 0.000559435\n",
      "Number of iterations: 874  loss: 0.00055894966\n",
      "Number of iterations: 875  loss: 0.00055846915\n",
      "Number of iterations: 876  loss: 0.00055799086\n",
      "Number of iterations: 877  loss: 0.0005575167\n",
      "Number of iterations: 878  loss: 0.00055704627\n",
      "Number of iterations: 879  loss: 0.0005565784\n",
      "Number of iterations: 880  loss: 0.00055611436\n",
      "Number of iterations: 881  loss: 0.0005556525\n",
      "Number of iterations: 882  loss: 0.0005551947\n",
      "Number of iterations: 883  loss: 0.0005547394\n",
      "Number of iterations: 884  loss: 0.00055428693\n",
      "Number of iterations: 885  loss: 0.00055383693\n",
      "Number of iterations: 886  loss: 0.00055338984\n",
      "Number of iterations: 887  loss: 0.00055294594\n",
      "Number of iterations: 888  loss: 0.00055250357\n",
      "Number of iterations: 889  loss: 0.0005520641\n",
      "Number of iterations: 890  loss: 0.0005516269\n",
      "Number of iterations: 891  loss: 0.0005511927\n",
      "Number of iterations: 892  loss: 0.0005507603\n",
      "Number of iterations: 893  loss: 0.00055033015\n",
      "Number of iterations: 894  loss: 0.00054990174\n",
      "Number of iterations: 895  loss: 0.00054947624\n",
      "Number of iterations: 896  loss: 0.0005490515\n",
      "Number of iterations: 897  loss: 0.00054863037\n",
      "Number of iterations: 898  loss: 0.0005482092\n",
      "Number of iterations: 899  loss: 0.0005477916\n",
      "Number of iterations: 900  loss: 0.0005473745\n",
      "Number of iterations: 901  loss: 0.0005469611\n",
      "Number of iterations: 902  loss: 0.0005465467\n",
      "Number of iterations: 903  loss: 0.000546137\n",
      "Number of iterations: 904  loss: 0.0005457251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 905  loss: 0.00054532074\n",
      "Number of iterations: 906  loss: 0.00054490945\n",
      "Number of iterations: 907  loss: 0.00054451084\n",
      "Number of iterations: 908  loss: 0.0005440997\n",
      "Number of iterations: 909  loss: 0.0005437089\n",
      "Number of iterations: 910  loss: 0.00054329407\n",
      "Number of iterations: 911  loss: 0.00054291607\n",
      "Number of iterations: 912  loss: 0.0005424924\n",
      "Number of iterations: 913  loss: 0.00054213713\n",
      "Number of iterations: 914  loss: 0.00054169615\n",
      "Number of iterations: 915  loss: 0.0005413878\n",
      "Number of iterations: 916  loss: 0.00054091716\n",
      "Number of iterations: 917  loss: 0.00054072565\n",
      "Number of iterations: 918  loss: 0.00054023525\n",
      "Number of iterations: 919  loss: 0.00054038846\n",
      "Number of iterations: 920  loss: 0.00054006715\n",
      "Number of iterations: 921  loss: 0.00054146815\n",
      "Number of iterations: 922  loss: 0.00054260826\n",
      "Number of iterations: 923  loss: 0.0005494596\n",
      "Number of iterations: 924  loss: 0.00055969844\n",
      "Number of iterations: 925  loss: 0.0005941616\n",
      "Number of iterations: 926  loss: 0.00065649697\n",
      "Number of iterations: 927  loss: 0.0008470116\n",
      "Number of iterations: 928  loss: 0.0011841964\n",
      "Number of iterations: 929  loss: 0.0022251434\n",
      "Number of iterations: 930  loss: 0.003424781\n",
      "Number of iterations: 931  loss: 0.005557552\n",
      "Number of iterations: 932  loss: 0.0043978654\n",
      "Number of iterations: 933  loss: 0.0017285242\n",
      "Number of iterations: 934  loss: 0.0005676979\n",
      "Number of iterations: 935  loss: 0.00079758425\n",
      "Number of iterations: 936  loss: 0.0008980707\n",
      "Number of iterations: 937  loss: 0.0006679087\n",
      "Number of iterations: 938  loss: 0.00055228616\n",
      "Number of iterations: 939  loss: 0.0005674886\n",
      "Number of iterations: 940  loss: 0.0005727977\n",
      "Number of iterations: 941  loss: 0.00056071597\n",
      "Number of iterations: 942  loss: 0.0005447268\n",
      "Number of iterations: 943  loss: 0.0005440719\n",
      "Number of iterations: 944  loss: 0.00054470723\n",
      "Number of iterations: 945  loss: 0.0005425773\n",
      "Number of iterations: 946  loss: 0.00054153916\n",
      "Number of iterations: 947  loss: 0.00054049026\n",
      "Number of iterations: 948  loss: 0.0005399279\n",
      "Number of iterations: 949  loss: 0.0005394294\n",
      "Number of iterations: 950  loss: 0.0005387499\n",
      "Number of iterations: 951  loss: 0.0005382496\n",
      "Number of iterations: 952  loss: 0.00053766125\n",
      "Number of iterations: 953  loss: 0.0005371592\n",
      "Number of iterations: 954  loss: 0.00053663395\n",
      "Number of iterations: 955  loss: 0.0005361389\n",
      "Number of iterations: 956  loss: 0.0005356433\n",
      "Number of iterations: 957  loss: 0.00053514086\n",
      "Number of iterations: 958  loss: 0.00053467473\n",
      "Number of iterations: 959  loss: 0.00053418335\n",
      "Number of iterations: 960  loss: 0.00053372653\n",
      "Number of iterations: 961  loss: 0.00053325633\n",
      "Number of iterations: 962  loss: 0.0005327982\n",
      "Number of iterations: 963  loss: 0.00053235085\n",
      "Number of iterations: 964  loss: 0.000531899\n",
      "Number of iterations: 965  loss: 0.0005314643\n",
      "Number of iterations: 966  loss: 0.00053102424\n",
      "Number of iterations: 967  loss: 0.000530596\n",
      "Number of iterations: 968  loss: 0.0005301692\n",
      "Number of iterations: 969  loss: 0.0005297465\n",
      "Number of iterations: 970  loss: 0.000529331\n",
      "Number of iterations: 971  loss: 0.00052891567\n",
      "Number of iterations: 972  loss: 0.0005285076\n",
      "Number of iterations: 973  loss: 0.00052810146\n",
      "Number of iterations: 974  loss: 0.0005276998\n",
      "Number of iterations: 975  loss: 0.0005273019\n",
      "Number of iterations: 976  loss: 0.00052690686\n",
      "Number of iterations: 977  loss: 0.00052651524\n",
      "Number of iterations: 978  loss: 0.0005261269\n",
      "Number of iterations: 979  loss: 0.0005257416\n",
      "Number of iterations: 980  loss: 0.0005253588\n",
      "Number of iterations: 981  loss: 0.0005249795\n",
      "Number of iterations: 982  loss: 0.0005246024\n",
      "Number of iterations: 983  loss: 0.0005242283\n",
      "Number of iterations: 984  loss: 0.00052385614\n",
      "Number of iterations: 985  loss: 0.0005234868\n",
      "Number of iterations: 986  loss: 0.00052312034\n",
      "Number of iterations: 987  loss: 0.0005227549\n",
      "Number of iterations: 988  loss: 0.00052239303\n",
      "Number of iterations: 989  loss: 0.00052203157\n",
      "Number of iterations: 990  loss: 0.0005216738\n",
      "Number of iterations: 991  loss: 0.0005213168\n",
      "Number of iterations: 992  loss: 0.00052096235\n",
      "Number of iterations: 993  loss: 0.0005206094\n",
      "Number of iterations: 994  loss: 0.0005202581\n",
      "Number of iterations: 995  loss: 0.00051990786\n",
      "Number of iterations: 996  loss: 0.0005195609\n",
      "Number of iterations: 997  loss: 0.0005192139\n",
      "Number of iterations: 998  loss: 0.00051887\n",
      "Number of iterations: 999  loss: 0.00051852583\n",
      "The train has finished\n",
      "The accuracy of this predict: 0.04199703700380752\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCcJNlAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181d3f7358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#训练\n",
    "# ---- 训练 ---- \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(1000):     #训练10个周期\n",
    "        #for batch in range( len(train)//batch_size  ): # 这个地方把原来的改掉了\n",
    "        for batch in range( len(x_train)//batch_size+1): # 1个周期内批次数\n",
    "            start = batch*batch_size \n",
    "            end = min( (batch+1)*batch_size , len(x_train)) \n",
    "            batch_xs, batch_ys =  x_train[start : end ], y_train[start : end ] # 获得本批次的x,y\n",
    "            _,loss_= sess.run([train_op, loss],feed_dict={X:batch_xs, Y:batch_ys}) # 训练\n",
    "        print(\"Number of iterations:\",epoch,\" loss:\",loss_)\n",
    "    print(\"The train has finished\")\n",
    "    # 开始测试\n",
    "    y_pred=[]\n",
    "    for x in  x_test :\n",
    "        y=sess.run(prediction,feed_dict={X:[x]})\n",
    "        y_pred.extend(y.reshape((-1)))\n",
    "    y_test=np.array(y_test)*std[7]+mean[7]\n",
    "    y_pred=np.array(y_pred)*std[7]+mean[7]\n",
    "    acc=np.average(np.abs(y_pred-y_test[:len(y_pred)])/y_test[:len(y_pred)])  # 精确度\n",
    "    print(\"The accuracy of this predict:\",acc)\n",
    "    #以折线图表示结果\n",
    "    plt.figure()\n",
    "    plt.plot(list(range(len(y_pred))), y_pred, color='b',)\n",
    "    plt.plot(list(range(len(y_test))), y_test,  color='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
